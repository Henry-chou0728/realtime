OpenAI Realtime API - 即時語音翻譯系統專案報告

1. #專案概述 (Project Overview)

    本專案實作了一個基於 OpenAI Realtime API (WebSocket) 的全雙工即時語音翻譯系統。系統能透過麥克風擷取使用者語音，並透過 GPT-4o-mini 模型即時轉寫並翻譯為英文。

    本系統的核心特色在於 「嚴格順序控制」，解決了串流傳輸中翻譯內容可能先於原文出現的非同步問題，確保使用者介面呈現符合閱讀邏輯（先顯示原文，再顯示譯文）。
<br>
2. #技術架構 (Technical Architecture)
#### 核心技術堆疊

- 通訊協定: WebSocket (wss://) - 實現低延遲雙向傳輸。

- 並發處理: asyncio - 同時處理音訊上傳與文字接收。

- 音訊處理: PyAudio - 負責 24kHz / 16-bit PCM 音訊流的實時擷取。

- 模型:  先用 whisper-1 進行語音識別將音訊轉換成文字，再使用gpt-realtime 進行中翻英。

#### 資料流架構

系統採用全雙工模式運作，資料流向如下：

- 上行 : 本地端持續將 Base64 編碼的音訊 Chunk 推送至 OpenAI Server。

- 處理 : Server 端進行 VAD (語音偵測)、STT (轉寫) 與 LLM 推論。

- 下行 : Server 非同步回傳 transcript (原文) 與 text.delta (翻譯)。

3. #關鍵邏輯 (Strict Order Logic)

為了避免即時串流中「翻譯跑得比原文快」導致的閱讀混亂，本程式碼實作了 Buffer Gate 機制，「以時間換取順序」的策略。它雖然可能會讓翻譯出現稍微慢個 0.1 ~ 0.5 秒（因為被擋了一下），但能確保使用者永遠是先看到「原文」，再看到「翻譯」，保證了介面的整潔與閱讀邏輯的正確性。

#### 運作流程
- 緩衝階段 : 當收到翻譯文字 但尚未收到完整原文時，系統將翻譯文字暫存於 self.buffered_translation，最後段落結束間隔1秒後 ("silence_duration_ms": 1000) 才會把轉錄文字的結果輸出。

- 檢查點 (Checkpoint) : 系統監聽 conversation.item.input_audio_transcription.completed 事件。一旦收到，確認原文已完整（印出 👂 [中文原文]: ......），並且翻譯過程要確認是否原文有確實輸入模型（印出 ✅ [檢查點]: 確認原文 ...... 已載入模型，準備翻譯...）。

- 釋放階段 : 原文確認後，即釋放緩衝區內的翻譯文字，隨後的串流內容則直接輸出（印出 🅰️ [英譯]: ......）。

#### 範例
- 輸入(聲音) : 翻譯模型 好厲害
- 輸出1 : 👂 [中文原文]: 翻譯模型 好厲害
- 輸出2 : ✅ [檢查點]: 確認原文 '翻譯模型 ...' 已載入模型，準備翻譯...
- 輸出3 : 🅰️  [英譯]: Wow, this translation model is really impressive!
![[Pasted image 20251217175245.png]]

#### 問題
- 當輸出文字太長時，若馬上接著下一段對話，模型可能無法即時翻譯前一句話就要跳到下一個翻譯任務，可能導致翻譯缺漏的問題。
- 目前沒有即時轉錄的功能。Gemini有。