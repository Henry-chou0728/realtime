# OpenAI Realtime API - 即時語音翻譯系統專案報告

## 1. 專案概述 (Project Overview)
本專案實作了一個基於 **OpenAI Realtime API (WebSocket)** 的全雙工即時語音翻譯系統。系統能透過麥克風擷取使用者語音，並透過 **GPT-4o-mini** 模型即時轉寫並翻譯為英文。

### 核心設計理念
本系統的核心特色在於 **「嚴格順序控制」**。我們解決了串流傳輸中翻譯內容可能先於原文出現的非同步問題，確保使用者介面呈現符合閱讀邏輯：**先顯示原文，再顯示譯文**。

---

## 2. 技術架構 (Technical Architecture)

### 核心技術堆疊
* **通訊協定**: WebSocket (`wss://`) — 實現低延遲雙向串流傳輸。
* **並發處理**: `asyncio` — 同時處理音訊上傳與文字接收，確保不阻塞。
* **音訊處理**: `PyAudio` — 負責 24kHz / 16-bit PCM 音訊流的實時擷取。
* **模型流程**: 
    1. 使用 **Whisper-1** 進行語音識別 (STT) 將音訊轉換成文字。
    2. 使用 **GPT-Realtime** 進行語義分析與中翻英任務。



### 資料流架構
系統採用全雙工模式運作，資料流向如下：
* **上行 (Uplink)**: 本地端持續將 Base64 編碼的音訊 Chunk 推送至 OpenAI Server。
* **處理 (Processing)**: Server 端進行 VAD (語音偵測)、STT (轉寫) 與 LLM 推論。
* **下行 (Downlink)**: Server 非同步回傳 `transcript` (原文) 與 `text.delta` (翻譯)。

---

## 3. 關鍵邏輯：嚴格順序邏輯 (Strict Order Logic)
為了避免即時串流中「翻譯跑得比原文快」導致的閱讀混亂，本系統實作了 **Buffer Gate (緩衝閘)** 機制，採取「以時間換取順序」的策略。

### 運作流程
1. **緩衝階段 (Buffer)**: 
   當收到翻譯文字但尚未收到完整原文時，系統將翻譯文字暫存於 `self.buffered_translation`。系統會等待段落結束間隔（`silence_duration_ms: 1000`）後才進行下一步。
   
2. **檢查點 (Checkpoint)**: 
   系統監聽 `conversation.item.input_audio_transcription.completed` 事件。一旦收到：
   * 印出：`👂 [中文原文]: ......`
   * 確認原文已載入模型並印出：`✅ [檢查點]: 確認原文 ...... 已載入模型，準備翻譯...`
   
3. **釋放階段 (Release)**: 
   原文確認後，立即釋放緩衝區內的翻譯文字，隨後的串流內容則直接輸出印出：`🅰️ [英譯]: ......`。



### 範例展示
* **輸入(聲音)**: 「翻譯模型好厲害」
* **輸出序列**:
  1. `👂 [中文原文]: 翻譯模型好厲害`
  2. `✅ [檢查點]: 確認原文 '翻譯模型 ...' 已載入模型，準備翻譯...`
  3. `🅰️ [英譯]: Wow, this translation model is really impressive!`

---

## 4. 當前問題與改進方向 (Issues & Challenges)

* **翻譯缺漏問題**: 
  當輸出文字太長時，若馬上接著下一段對話，模型可能為了追趕新的任務而跳過前一句話的翻譯，導致內容不完整。
* **即時轉錄限制**: 
  目前的架構在「說話同時顯示字體」的即時性上仍有優化空間（目前需等待 1 秒靜音）。相較之下，Gemini 在即時轉錄的反饋速度上表現更為卓越。

---
